
class: middle, center, title-slide 

# –°–∏—Å—Ç–µ–º–∏ —à—Ç—É—á–Ω–æ–≥–æ —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É

–õ–µ–∫—Ü—ñ—è 1-2: –í—Å—Ç—É–ø –¥–æ —à—Ç—É—á–Ω–æ–≥–æ —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É

<br><br>
–ö–æ—á—É—Ä–∞ –Æ—Ä—ñ–π –ü–µ—Ç—Ä–æ–≤–∏—á<br>
[iuriy.kochura@gmail.com](mailto:iuriy.kochura@gmail.com) <br>
<a href="https://t.me/y_kochura">@y_kochura</a> <br>


---

class: middle

# –°—å–æ–≥–æ–¥–Ω—ñ

- –Ü–Ω—Ç–µ–ª–µ–∫—Ç vs —à—Ç—É—á–Ω–∏–π —ñ–Ω—Ç–µ–ª–µ–∫—Ç
- –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è —à—Ç—É—á–Ω–æ–≥–æ —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É —Ç–∞ –ø–∞—Ä–∞–¥–∏–≥–º–∞
- –¢–∏–ø–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è
- –ö–æ–Ω—Ü–µ–ø—Ü—ñ—è –≥–ª–∏–±–∏–Ω–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è
- –ü—Ä–∏–∫–ª–∞–¥–∏ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –≥–ª–∏–±–∏–Ω–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è
- –ü–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω: –ø—Ä—è–º–µ —Ç–∞ –∑–≤–æ—Ä–æ—Ç–Ω–µ –ø–æ—à–∏—Ä–µ–Ω–Ω—è
- –ó–∞–≥–∞–ª—å–Ω—ñ —Ñ—É–Ω–∫—Ü—ñ—ó –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó

---


class: blue-slide, middle, center
count: false

.larger-xx[–®—Ç—É—á–Ω–∏–π —ñ–Ω—Ç–µ–ª–µ–∫—Ç]

---

class: middle

# –ß–∏ –º–æ–∂–µ –º–∞—à–∏–Ω–∞ –¥—É–º–∞—Ç–∏?
.grid[
.kol-2-3[
.width-90[![](figures/lec1/computing-machinery-and-intelligence.jpg)]

.pull-right[&mdash; Alan Turing, 1950]
]

.kol-1-3[.center.circle.width-70[![](figures/lec1/turing.jpg)]

.center.smaller-xxx[Image source: [biography](https://www.biography.com/scientist/alan-turing)]
  ]
]

.footnote[Credits: [Alan Turing](https://academic.oup.com/mind/article/LIX/236/433/986238), 1950.]

---


class: middle
count: false


.smaller-x.italic[
In the process of trying to imitate an adult human mind we are bound to think a good deal about
the process which has brought it to the state that it is in. We may notice three components,

  a. The initial state of the mind, say at birth,

  b. The education to which it has been subjected,

  c. Other experience, not to be described as education, to which it has been subjected.

Instead of trying to produce a programme to simulate the adult mind, why not rather try to produce
one which simulates the child‚Äôs? If this were then subjected to an appropriate course of education one
would obtain the adult brain. Presumably the child-brain is something like a note-book as one buys
it from the stationers. Rather little mechanism, and lots of blank sheets. (Mechanism and writing
are from our point of view almost synonymous.) Our hope is that there is so little mechanism in
the child-brain that something like it can be easily programmed.

]

.pull-right[&mdash; Alan Turing, 1950]

.footnote[Credits: [Alan Turing](https://academic.oup.com/mind/article/LIX/236/433/986238), 1950.]

---

class: middle

# –©–æ —Ç–∞–∫–µ —ñ–Ω—Ç–µ–ª–µ–∫—Ç?

- –Ü–Ω—Ç–µ–ª–µ–∫—Ç &mdash; —Ü–µ –ø—Ä–æ –∑–¥–∞—Ç–Ω—ñ—Å—Ç—å

.bold.center.larger-x[–Ω–∞–≤—á–∞—Ç–∏—Å—è –ø—Ä–∏–π–º–∞—Ç–∏ —Ä—ñ—à–µ–Ω–Ω—è –¥–ª—è –¥–æ—Å—è–≥–Ω–µ–Ω–Ω—è —Ü—ñ–ª–µ–π]
   

- –ù–∞–≤—á–∞–Ω–Ω—è, –ø—Ä–∏–π–Ω—è—Ç—Ç—è —Ä—ñ—à–µ–Ω–Ω—è, —Ç–∞ —Ü—ñ–ª—ñ —î –∫–ª—é—á–æ–≤–∏–º–∏

---

class: middle

# –©–æ —Ç–∞–∫–µ —à—Ç—É—á–Ω–∏–π —ñ–Ω—Ç–µ–ª–µ–∫—Ç?

- –£ —à–∏—Ä–æ–∫–æ–º—É —Å–µ–Ω—Å—ñ 

.bold.larger-x[–ë—É–¥—å-—è–∫–∞ —Ç–µ—Ö–Ω—ñ–∫–∞, —è–∫–∞ –¥–æ–∑–≤–æ–ª—è—î –∫–æ–º–ø'—é—Ç–µ—Ä–∞–º —ñ–º—ñ—Ç—É–≤–∞—Ç–∏ –ø–æ–≤–µ–¥—ñ–Ω–∫—É –ª—é–¥–∏–Ω–∏]
   
---

class: middle

# –©–æ —Ç–∞–∫–µ —à—Ç—É—á–Ω–∏–π —ñ–Ω—Ç–µ–ª–µ–∫—Ç?

- –£ –≤—É–∑—å–∫–æ–º—É —Å–µ–Ω—Å—ñ 

.alert[
.bold.larger-x[**–®—Ç—É—á–Ω–∏–π —ñ–Ω—Ç–µ–ª–µ–∫—Ç** &mdash; –∑–¥–∞—Ç–Ω—ñ—Å—Ç—å —ñ–Ω–∂–µ–Ω–µ—Ä–Ω–æ—ó —Å–∏—Å—Ç–µ–º–∏ –æ–±—Ä–æ–±–ª—è—Ç–∏, –∑–∞—Å—Ç–æ—Å–æ–≤—É–≤–∞—Ç–∏ —Ç–∞ –≤–¥–æ—Å–∫–æ–Ω–∞–ª—é–≤–∞—Ç–∏ –∑–¥–æ–±—É—Ç—ñ –∑–Ω–∞–Ω–Ω—è —Ç–∞ –≤–º—ñ–Ω–Ω—è.]]

- **–ó–Ω–∞–Ω–Ω—è** &mdash; —Ü–µ —Ñ–∞–∫—Ç–∏, —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è —Ç–∞ –Ω–∞–≤–∏—á–∫–∏, –Ω–∞–±—É—Ç—ñ —á–µ—Ä–µ–∑ –¥–æ—Å–≤—ñ–¥ –∞–±–æ –Ω–∞–≤—á–∞–Ω–Ω—è.

.footnote[Credits: [ISO/IEC TR 24028:2020(en)](https://www.iso.org/obp/ui/#iso:std:iso-iec:tr:24028:ed-1:v1:en:term:3.4), 2020.]

---

class: middle

## –ö–æ—Ä–æ—Ç–∫–∞ —ñ—Å—Ç–æ—Ä—ñ—è

.smaller-xx[
- 1940‚Äî1952: Early days
  - 1943: McCulloch & Pitts: Boolean circuit model of brain
  - 1950: Turing's ''Computing Machinery and Intelligence''

- 1952‚Äì1956:  The birth of AI
  - 1950s: Early AI programs, including Samuel's checkers program,
Newell & Simon's Logic Theorist, Gelernter's Geometry Engine
  - 1956: Dartmouth meeting: ''Artificial Intelligence'' adopted

- 1956‚Äì1974: The golden years 
  - 1958: Frank Rosenblatt invented [perceptron](https://en.wikipedia.org/wiki/Perceptron) (simple neural network)
  - 1964: [Bobrow's program](https://en.wikipedia.org/wiki/STUDENT_(computer_program) that solves algebra word problems
  - 1965: Robinson's complete algorithm for logical reasoning

- 1974‚Äì1980: The first AI winter

- 1980‚Äì1987: Expert systems industry boom
- 1987‚Äî1993: Expert systems industry busts: the second AI winter 

- 1993‚Äì2011: Statistical approaches 
  - Resurgence of probability, focus on uncertainty
  - General increase in technical depth
  - Intelligent agents

- 2011‚Äìpresent: Deep Learning, Big Data and AI
  - Big data, big compute, neural networks
  - AI used in many industries
]

.footnote[Credits: [Wikipedia - History of artificial intelligence](https://en.wikipedia.org/wiki/History_of_artificial_intelligence#Deep_learning)]

---

class: middle

# AI &mdash; –±–∞–≥–∞—Ç–∞ –≥–∞–ª—É–∑—å

.center.width-90[![](figures/lec1/Fields-of-artificial-intelligence-10.png)]

.footnote[Image Source: [Marizel B. and Ma. Louella Salenga](https://www.researchgate.net/publication/324183626_Bitter_Melon_Crop_Yield_Prediction_using_Machine_Learning_Algorithm), 2018.]

---

class: middle

.center.width-90[![](figures/lec1/ML-capabilities.png)]

.footnote[Image Source: [Why you Might Want to use Machine Learning](https://ml-ops.org/content/motivation).]

---

class: middle

.center.width-50[![](figures/lec1/AndrewNG.webp)]

"Just as electricity transformed almost everything 100 years ago, today I actually have a hard time thinking of an industry that I don't think AI will transform in the next several years."

.pull-right[&mdash; Andrew Ng]

.footnote[Credits: [Andrew Ng: Artificial Intelligence is the New Electricity](https://www.youtube.com/watch?v=21EiKfQYZXc), 2017.]

---

class: blue-slide, middle, center
count: false

.larger-xx[–ú–∞—à–∏–Ω–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è]

---

class: middle

# –©–æ —Ç–∞–∫–µ –º–∞—à–∏–Ω–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è?

.grid[
.kol-2-3[
.width-90[![](figures/lec1/Some-Studies-in-Machine-Learnin.png)]

Field of study that gives computers the ability to learn without being explicitly programmed.
.pull-right[&mdash; Arthur Samuel, 1959]
]

.kol-1-3[.center.circle.width-60[![](figures/lec1/samuel.jpg)]

.center.smaller-xxx[Image source: [wikipedia](https://en.wikipedia.org/wiki/Arthur_Samuel#/media/File:This_is_the_photo_of_Arthur_Samuel.jpg)]
  ]
]

.footnote[Credits: [Arthur Samuel](http://users.auth.gr/kehagiat/Research/GameTheory/12CombBiblio/Checkers_Samuels_ibmrd1106C.pdf), 1959.]

---

class: middle

# –©–æ —Ç–∞–∫–µ –º–∞—à–∏–Ω–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è?

.grid[
.kol-2-3[

Machine Learning is the study of computer algorithms that improve automatically through experience.

.pull-right[&mdash; Tom Mitchell, 1997]
]

.kol-1-3[.center.width-100[![](figures/lec1/mitchell.jpg)]

.center.smaller-xxx[Image source: [Tom Mitchell's Home Page](http://www.cs.cmu.edu/~tom/)]
  ]
]

.footnote[Credits: [Tom Mitchell](http://www.cs.cmu.edu/~tom/mlbook.html), 1997.]

---

class: middle

# –ú–∞—à–∏–Ω–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è: –Ω–æ–≤–∞ –ø–∞—Ä–∞–¥–∏–≥–º–∞ –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è

.center.width-90[![](figures/lec1/mlvp.png)]

---

class: middle

# –¢–∏–ø–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è

.center[
.width-100[![](figures/lec1/typesLearning.png)]
]

---

class: middle

# What is Deep Learning?

.center.width-90[![](figures/lec1/ai.png)]

.footnote[Slide source: [MIT 6.S191](http://introtodeeplearning.com/)]

---

class: middle

# –ß–æ–º—É –≥–ª–∏–±–∏–Ω–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è (DL)?

–û–∑–Ω–∞–∫–∏, —Ä–æ–∑—Ä–æ–±–ª–µ–Ω—ñ –≤—Ä—É—á–Ω—É, –∑–∞–π–º–∞—é—Ç—å –±–∞–≥–∞—Ç–æ —á–∞—Å—É, —î –∫—Ä–∏—Ö–∫–∏–º–∏ —Ç–∞ –Ω–µ –ø—ñ–¥–ª—è–≥–∞—é—Ç—å –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—é –Ω–∞ –ø—Ä–∞–∫—Ç–∏—Ü—ñ

–ß–∏ –º–æ–∂–µ–º–æ –º–∏ –≤–∏–≤—á–∏—Ç–∏ **–æ—Å–Ω–æ–≤–Ω—ñ –æ–∑–Ω–∞–∫–∏** –±–µ–∑–ø–æ—Å–µ—Ä–µ–¥–Ω—å–æ –∑ –¥–∞–Ω–∏—Ö?

.center.width-90[![](figures/lec1/features.png)]

.footnote[Slide source: [MIT 6.S191](http://introtodeeplearning.com/)]

---


# –ß–æ–º—É DL –ø—Ä–∞—Ü—é—î –∑–∞—Ä–∞–∑?

.grid[
.kol-1-2[
Algorithms (old and new)<br><br>
.width-90[![](figures/lec1/skip-connection.png)]
]
.kol-1-2[
More data
.smaller-xx[
- Larger Datasets
- Easier Collection & Storage
]

.center.width-50[![](figures/lec1/imagenet.jpeg)]
]
]

.center.grid[
.kol-1-2[
Software (New models and improved techniques)<br>
.width-90[![](figures/lec1/software.png)]
]
.kol-1-2[
Faster compute engines<br><br>
.width-50[![](figures/lec1/titan.jpg)]
]
]

???

The success of deep learning is multi-factorial...

---

class: middle

## DL —è–∫ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–Ω–∞ –º–æ–≤–∞

.width-100[![](figures/lec1/lego-composition.png)]

.footnote[Image source: [http://chelseamarzean.com/post-the-atomic-workflow/](http://chelseamarzean.com/post-the-atomic-workflow/), 2016.]

---

class: middle

.center.circle.width-30[![](figures/lec1/bishop.jpg)]

.center.smaller-xxx[Image source: [Christopher Bishop](https://www.microsoft.com/en-us/research/people/cmbishop/)]

.italic[For the last forty years we have programmed computers; for the next forty years we will train them.]

.pull-right[&mdash; Chris Bishop, 2020.]

.footnote[Credits: [Chris Bishop on "The Real AI Revolution" NeurIPS 2020](https://www.ml6.eu/knowhow/ml6-at-neurips-2020)]

---

class: middle 

.center.width-70[![](figures/lec1/turing-award.png)]

.italic["ACM named .bold[Yann LeCun], .bold[Geoffrey Hinton], and .bold[Yoshua Bengio] recipients of the .bold[2018 ACM A.M. Turing Award] for conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing."]

.footnote[Credits: [Wikipedia](https://en.wikipedia.org/wiki/Turing_Award)]

---


class: blue-slide, middle, center
count: false

.larger-xx[–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è —Ç–∞ —É—Å–ø—ñ—Ö–∏]

---


class: middle, center, black-slide

<iframe width="600" height="450" src="https://www.youtube.com/embed/5kpsZoKjPgQ" frameborder="0" allowfullscreen></iframe>

Object detection, pose estimation, segmentation (2019)

---

class: middle, center, black-slide

<iframe width="600" height="450" src="https://www.youtube.com/embed/V1eYniJ0Rnk" frameborder="0" allowfullscreen></iframe>

Reinforcement learning (Mnih et al, 2014)

---

class: middle, center, black-slide

<iframe width="600" height="450" src="https://www.youtube.com/embed/HcZ48JDamyk" frameborder="0" allowfullscreen></iframe>

Strategy games (Deepmind, 2016-2018)

---

class: middle, center, black-slide

<iframe width="600" height="450" src="https://www.youtube.com/embed/qhUvQiKec2U" frameborder="0" allowfullscreen></iframe>

Autonomous cars (NVIDIA, 2016)

---

class: middle, center, black-slide

<iframe width="600" height="450" src="https://www.youtube.com/embed/tlThdr3O5Qo" frameborder="0" allowfullscreen></iframe>

Autopilot (Tesla, 2019)

???

A full build of Autopilot neural networks involves 48 networks that take 70,000 GPU hours to train üî•. Together, they output 1,000 distinct tensors (predictions) at each timestep.

---

class: middle, black-slide

.center[
<video loop controls preload="auto" height="400" width="600">
  <source src="./figures/lec1/physics-simulation.mp4" type="video/mp4">
</video>

Physics simulation (Sanchez-Gonzalez et al, 2020)

]

---

class: middle, black-slide, center

<iframe width="600" height="450" src="https://www.youtube.com/embed/gg7WjuFs8F4" frameborder="0" allowfullscreen></iframe>

AI for Science (Deepmind, AlphaFold, 2020)

---

class: middle, center, black-slide

<iframe width="600" height="450" src="https://www.youtube.com/embed/7gh6_U7Nfjs" frameborder="0" allowfullscreen></iframe>

Speech synthesis and question answering (Google, 2018)

---

class: middle, center, black-slide

<iframe width="600" height="450" src="https://www.youtube.com/embed/Khuj4ASldmU" frameborder="0" allowfullscreen></iframe>

Artistic style transfer (Ruder et al, 2016)

---

class: middle, center, black-slide

<iframe width="600" height="450" src="https://www.youtube.com/embed/kSLJriaOumA" frameborder="0" allowfullscreen></iframe>

Image generation (Karras et al, 2018)

---

class: middle, center, black-slide

<iframe width="600" height="450" src="https://www.youtube.com/embed/egJ0PTKQp4U?start=223" frameborder="0" allowfullscreen></iframe>

Music composition (NVIDIA, 2017)

---

class: middle, center, black-slide

<iframe width="600" height="450" src="https://www.youtube.com/embed/BIDaxl4xqJ4" frameborder="0" allowfullscreen></iframe>

Dali Lives (2019)

---

class: middle, center, black-slide

<iframe width="600" height="450" src="https://www.youtube.com/embed/J_2fIGmsoRg" frameborder="0" allowfullscreen></iframe>

Reface revived the famous Kyiv murals for Kyiv Day (2021)

---


class: blue-slide, middle, center
count: false

.larger-xx[–ü–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω]

–°—Ç—Ä—É–∫—Ç—É—Ä–Ω–∏–π –±—É–¥—ñ–≤–µ–ª—å–Ω–∏–π –±–ª–æ–∫ –≥–ª–∏–±–æ–∫–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è

---

# –ü–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω

–ú–æ–¥–µ–ª—å –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω–∞ (Rosenblatt, 1958)

$$g(z) = \begin{cases}
   1 &\text{if } z =\sum_i w_i x_i + b \geq 0  \\\\
   0 &\text{otherwise}
\end{cases}$$

–¶—è –º–æ–¥–µ–ª—å —Å–ø–æ—á–∞—Ç–∫—É –±—É–ª–∞ –º–æ—Ç–∏–≤–æ–≤–∞–Ω–∞ –±—ñ–æ–ª–æ–≥—ñ—î—é, –¥–µ $w_i$ &mdash; —Å–∏–Ω–∞–ø—Ç–∏—á–Ω—ñ –≤–∞–≥–∏ –¥–ª—è –≤—Ö—ñ–¥–Ω–∏—Ö —Å–∏–≥–Ω–∞–ª—ñ–≤ $x_i$ —Ç–∞ $g$ &mdash; –∞–∫—Ç–∏–≤–∞—Ü—ñ—è –Ω–µ–π—Ä–æ–Ω–∞.
.center.width-65[![](figures/lec1/perceptron.jpg)]

.footnote[Image source: Frank Rosenblatt, [Mark I Perceptron operators' manual](https://apps.dtic.mil/sti/pdfs/AD0236965.pdf), 1960.]

???

In November 1958 Frank Rosenblatt invented the Perceptron, or Mark I, at Cornell University. Completed in 1960, this was the first computer that could learn new skills by trial and error, using a type of neural network that simulated human thought processes.

---


class: middle

# –ë—ñ–æ–ª–æ–≥—ñ—á–Ω–∏–π vs —à—Ç—É—á–Ω–∏–π –Ω–µ–π—Ä–æ–Ω 

.center[
.width-100[![](figures/lec1/NeuronBioMathModels.png)]
]

---


class: middle

.center[
.width-100[![](figures/lec1/p1.png)]
]

.footnote[Slide source: [MIT 6.S191](http://introtodeeplearning.com/)]

---


class: middle

.center[
.width-100[![](figures/lec1/p2.png)]
]

.footnote[Slide source: [MIT 6.S191](http://introtodeeplearning.com/)]

---

class: middle

.center[
.width-100[![](figures/lec1/p3.png)]
]

.footnote[Slide source: [MIT 6.S191](http://introtodeeplearning.com/)]

---


class: middle

.center[
.width-100[![](figures/lec1/p4.png)]
]

.footnote[Slide source: [MIT 6.S191](http://introtodeeplearning.com/)]

---

class: middle

.center[
.width-70[![](figures/lec1/neuron.png)]
]

.smaller-xx[
$$
\begin{aligned}
\mathbf{X} = \begin{bmatrix}
x\_1  \\\\
x\_2  \\\\
\vdots \\\\
x\_m
\end{bmatrix} 
&&
\mathbf{W} = \begin{bmatrix}
w\_1  \\\\
w\_2  \\\\
\vdots \\\\
w\_m
\end{bmatrix}
&& 
\mathbf{X}^T = \begin{bmatrix}
x\_1 & x\_2 & \cdots & x\_m
\end{bmatrix} 
\end{aligned}$$


$$\boxed{\begin{aligned}z &= \sum\_{n=1}^{m} w\_n x\_n + b = \mathbf{X}^T \cdot \mathbf{W} + b = \mathbf{W}^T \cdot \mathbf{X} + b \\\\
\hat y &= g(z) \\\\
\mathcal{L}(\hat y, y) &= - \frac{1}{n} \sum\_{i=1}^{n} \big(y^{(i)} \log(\hat y^{(i)}) + (1- y^{(i)}) \log(1 -\hat y^{(i)}) \big)
\end{aligned}}$$

]

---

class: middle

.center[
.width-80[![](figures/lec1/neuron.png)]
]

.smaller-xx[

.center[*–ü—Ä—è–º–µ –ø–æ—à–∏—Ä–µ–Ω–Ω—è*]

$$\boxed{\begin{aligned}z &= \sum\_{n=1}^{m} w\_n x\_n + b = \mathbf{X}^T \cdot \mathbf{W} + b = \mathbf{W}^T \cdot \mathbf{X} + b \\\\
\hat y &= g(z) \\\\
\mathcal{L}(\hat y, y) &= - \frac{1}{n} \sum\_{i=1}^{n} \big(y^{(i)} \log(\hat y^{(i)}) + (1- y^{(i)}) \log(1 -\hat y^{(i)}) \big)
\end{aligned}}$$

]

---


class: middle

# –ó–∞–≥–∞–ª—å–Ω—ñ —Ñ—É–Ω–∫—Ü—ñ—ó –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó

.center[
.width-100[![](figures/lec1/actFunctions.png)]
]

---

class: middle

.center[
.width-100[![](figures/lec1/p5.png)]
]

.footnote[Slide source: [MIT 6.S191](http://introtodeeplearning.com/)]

---

class: middle

.center[
.width-100[![](figures/lec1/p6.png)]
]

.footnote[Slide source: [MIT 6.S191](http://introtodeeplearning.com/)]

---

class: middle

.center[
.width-100[![](figures/lec1/p7.png)]
]

.footnote[Slide source: [MIT 6.S191](http://introtodeeplearning.com/)]

---

class: middle

.center[
.width-100[![](figures/lec1/p8.png)]
]

.footnote[Slide source: [MIT 6.S191](http://introtodeeplearning.com/)]

---

class: middle

.center[
.width-100[![](figures/lec1/p9.png)]
]

.footnote[Slide source: [MIT 6.S191](http://introtodeeplearning.com/)]

---

class: middle

.center[
.width-100[![](figures/lec1/p10.png)]
]

.footnote[Slide source: [MIT 6.S191](http://introtodeeplearning.com/)]

---

class: middle

.center[
.width-100[![](figures/lec1/p11.png)]
]

.footnote[Slide source: [MIT 6.S191](http://introtodeeplearning.com/)]

---

class: middle

.center[
.width-100[![](figures/lec1/p12.png)]
]

.footnote[Slide source: [MIT 6.S191](http://introtodeeplearning.com/)]

---


class: middle

## –ü—Ä–∏–∫–ª–∞–¥ 2

–ü—Ä–∏–ø—É—Å—Ç–∏–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –≤—Ö—ñ–¥–Ω–∏—Ö –æ–∑–Ω–∞–∫ $m = 3$

$$
\begin{aligned}
\mathbf{X} = \begin{bmatrix}
x\_1  \\\\
x\_2  \\\\
x\_3
\end{bmatrix} = \begin{bmatrix}
-0.1  \\\\
0.7  \\\\
0.5
\end{bmatrix} 
&&
\mathbf{W} = \begin{bmatrix}
w\_1  \\\\
w\_2  \\\\
w\_3
\end{bmatrix} =
\begin{bmatrix}
1  \\\\
-2  \\\\
2
\end{bmatrix}
&&
b = 0.8
\end{aligned}$$

$$\boxed{\begin{aligned}
z = \sum_{n=1}^{3} w_n x_n + b &= w_1 x_1 + w_2 x_2 + w_3 x_3 + b = \\\\
&= 1 \cdot -0.1 + -2 \cdot 0.7 + 2 \cdot 0.5 + 0.8 = 0.3
\end{aligned}}$$

$$\boxed{\begin{aligned}
z = \mathbf{X}^T \cdot \mathbf{W} + b &= \begin{bmatrix}
x\_1 & x\_2 &  x\_3 
\end{bmatrix} \begin{bmatrix}
w\_1  \\\\
w\_2  \\\\
w\_3
\end{bmatrix} + b = \\\\
&= w_1 x_1 + w_2 x_2 + w_3 x_3 + b = 0.3
\end{aligned}}$$


$$\hat y  = g(z) = g(\mathbf{X}^T \cdot \mathbf{W} + b) = \frac{1}{1 + \exp(-z)} = \frac{1}{1 + \exp(-0.3)} \approx 0.57 $$

---

class: blue-slide, middle, center
count: false

.larger-xx[–û–¥–Ω–æ–≤–∏–º—ñ—Ä–Ω–∏–π –≥—Ä–∞–¥—ñ—î–Ω—Ç–Ω–∏–π —Å–ø—É—Å–∫]

---

class: middle

## –û–¥–Ω–æ–≤–∏–º—ñ—Ä–Ω–∏–π –≥—Ä–∞–¥—ñ—î–Ω—Ç–Ω–∏–π —Å–ø—É—Å–∫
.smaller-x[

–†–æ–∑–≥–ª—è–Ω–µ–º–æ –¥–µ—è–∫—É –Ω–µ–ø–µ—Ä–µ—Ä–≤–Ω—É –¥–∏—Ñ–µ—Ä–µ–Ω—Ü—ñ–π–æ–≤–∞–Ω—É –¥—ñ–π—Å–Ω—É —Ñ—É–Ω–∫—Ü—ñ—é $f: \mathbb{R} \rightarrow \mathbb{R}$. –†–æ–∑–∫–ª–∞–¥–∞—é—á–∏ —É —Ä—è–¥ –¢–µ–π–ª–æ—Ä–∞, –º–∏ –æ—Ç—Ä–∏–º—É—î–º–æ::

$$f(x + \varepsilon) = f(x) + \varepsilon f^{'}(x) + \mathcal{O}(\varepsilon^2)$$

–©–æ–± —É—Å–µ –±—É–ª–æ –ø—Ä–æ—Å—Ç–æ, –¥–∞–≤–∞–π—Ç–µ –≤–∏–±–µ—Ä–µ–º–æ —Ñ—ñ–∫—Å–æ–≤–∞–Ω–∏–π —Ä–æ–∑–º—ñ—Ä –∫—Ä–æ–∫—É $\alpha > 0$ —Ç–∞ –æ–±–µ—Ä–µ–º–æ $\varepsilon = -\alpha f^{'}(x)$. –ü—ñ–¥—Å—Ç–∞–≤–ª—è—é—á–∏ —Ü–µ —É —Ä—è–¥ –¢–µ–π–ª–æ—Ä–∞, –æ—Ç—Ä–∏–º–∞—î–º–æ:

$$f(x -\alpha f^{'}(x)) = f(x) - \alpha f^{'2}(x)  + \mathcal{O}(\alpha^2 f^{'2}(x))$$

–Ø–∫—â–æ –ø–æ—Ö—ñ–¥–Ω–∞ $f^{'}(x) \neq 0$ –Ω–µ –∑–Ω–∏–∫–∞—î –º–∏ —Ä–æ–±–∏–º–æ –ø—Ä–æ–≥—Ä–µ—Å —Ç–∞–∫ —è–∫ $\alpha f^{'2}(x) > 0$. –ö—Ä—ñ–º —Ç–æ–≥–æ, –º–∏ –∑–∞–≤–∂–¥–∏ –º–æ–∂–µ–º–æ –≤–∏–±—Ä–∞—Ç–∏ $\alpha$ –¥–æ—Å–∏—Ç—å –º–∞–ª–∏–º, —â–æ–± –≤–∏—Ä–∞–∑–∏ –≤–∏—â–∏—Ö –ø–æ—Ä—è–¥–∫—ñ–≤ —Å—Ç–∞–ª–∏ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏–º–∏. –¢–æ–º—É –º–∏ –ø—Ä–∏—Ö–æ–¥–∏–º–æ –¥–æ

$$f(x -\alpha f^{'}(x)) \lessapprox f(x)$$

–¶–µ –æ–∑–Ω–∞—á–∞—î, —è–∫—â–æ –º–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ

$$x \leftarrow x -\alpha f^{'}(x)$$

–¥–ª—è —ñ—Ç–µ—Ä–∞—Ü—ñ—ó –ø–æ $x$, –∑–Ω–∞—á–µ–Ω–Ω—è —Ñ—É–Ω–∫—Ü—ñ—ó  $f(x)$  –º–æ–∂–µ –∑–º–µ–Ω—à–∏—Ç–∏—Å—å. 
]

???
Gradient descent in one dimension is an excellent example to explain why the gradient descent algorithm may reduce the value of the objective function.

The Taylor series is used to describe what the function looks like in the neighborhood of some poin $x$.

That is, in first-order approximation $f(x + \varepsilon)$  is given by the function value $f(x)$ and the first derivative $f^{'}(x)$ at $x$. It is not unreasonable to assume that for small $\varepsilon$ moving in the direction of the negative gradient will decrease $f$. 

Therefore, in gradient descent we first choose an initial value $x$ and a constant $\alpha > 0$ and then use them to continuously iterate $x$ until the stop condition is reached, for example, when the magnitude of the gradient $|f^{'}(x)|$ is small enough or the number of iterations has reached a certain value.

---

class: middle

.center[
.width-80[![](figures/lec1/gdC.png)]
]

???

For simplicity we choose the objective function $f(x) = x^2$ to illustrate how to implement gradient descent. Although we know that $x = 0$ is the solution to minimize $f(x)$, we still use this simple function to observe how $x$ changes.

---

class: middle

The progress of optimizing over $x$ 

.center[
.width-80[![](figures/lec1/gd025.png)]
]

---

class: middle

The progress of optimizing over $x$ 

.center[
.width-80[![](figures/lec1/gd006.png)]
]

???
If we use a learning rate that is too small, it will cause $x$ to update very slowly, requiring more iterations to get a better solution.

---

lass: middle

The progress of optimizing over $x$ 

.center[
.width-80[![](figures/lec1/gd1.1.png)]
]

???
if we use an excessively high learning rate, $|\alpha f^{'}(x)|$ might be too large for the first-order Taylor expansion formula. That is, the term $\mathcal{O}(\alpha^2 f^{'2}(x))$ might become significant. In this case, we cannot guarantee that the iteration of $x$ will be able to lower the value of $f(x)$.

---

class: blue-slide, middle, center
count: false

.larger-xx[–ü–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω: –ó–≤–æ—Ä–æ—Ç–Ω–µ –ø–æ—à–∏—Ä–µ–Ω–Ω—è]

---

class: middle

In Leibniz notations, the **chain rule** states that
$$
\begin{aligned}
\frac{\partial \ell}{\partial \theta\_i} &= \sum\_{k \in \text{parents}(\ell)} \frac{\partial \ell}{\partial u\_k} \underbrace{\frac{\partial u\_k}{\partial \theta\_i}}\_{\text{recursive case}}
\end{aligned}$$

---

class: middle

## Backpropagation

- Since a neural network is a **composition of differentiable functions**, the total
derivatives of the loss can be evaluated backward, by applying the chain rule
recursively over its computational graph.
- The implementation of this procedure is called reverse *automatic differentiation* or **backpropagation**.

---

class: middle



.smaller-xx[

.center[*Forward propagation*]

$$\boxed{\begin{aligned}z &= \sum\_{n=1}^{m} w\_n x\_n + b = \mathbf{X}^T \cdot \mathbf{W} + b = \mathbf{W}^T \cdot \mathbf{X} + b \\\\
\hat y &= g(z) = \sigma(z) = \frac{1}{1 + \exp(-z)} \\\\
\mathcal{L}(\hat y, y) &= - \frac{1}{n} \sum\_{i=1}^{n} \big(y^{(i)} \log(\hat y^{(i)}) + (1- y^{(i)}) \log(1 -\hat y^{(i)}) \big)
\end{aligned}}$$


.grid[
.kol-2-3[

.center[*Backward propagation*]

$$\boxed{\begin{aligned}
\frac{\partial \mathcal{L}(\hat y, y)}{\partial \hat y} &= -\frac{y}{\hat y} + \frac{1- y}{1 - \hat y} \\\\[18pt]
\frac{\partial \mathcal{L}(\hat y, y)}{\partial z} &= \frac{\partial \mathcal{L}(\hat y, y)}{\partial \hat y} \frac{\partial \hat y}{\partial z} = \hat y - y \\\\[18pt]
\frac{\partial \mathcal{L}(\hat y, y)}{\partial \mathbf{W}} &= \frac{\partial \mathcal{L}(\hat y, y)}{\partial \hat y} \frac{\partial \hat y}{\partial z} \frac{\partial z}{\partial \mathbf{W}} = \mathbf{X}^T \cdot (\hat y - y) \\\\[18pt]
\frac{\partial \mathcal{L}(\hat y, y)}{\partial b} &=  \frac{\partial \mathcal{L}(\hat y, y)}{\partial \hat y} \frac{\partial \hat y}{\partial z} \frac{\partial z}{\partial b} = \hat y - y
\end{aligned}}$$
]

.kol-1-3[
.center[*Update the weights and bias*]

$$\boxed{\begin{aligned}
\mathbf{W} &= \mathbf{W} - \alpha \frac{\partial \mathcal{L}(\hat y, y)}{\partial \mathbf{W}} \\\\[18pt]
b &= b - \alpha \frac{\partial \mathcal{L}(\hat y, y)}{\partial b}
\end{aligned}}$$
]]
]

---

class: blue-slide, middle, center
count: false

.larger-xx[Multi Output Perceptron]

---

class: middle

# Multi Output Perceptron

.smaller-x[Because all inputs are densely connected to all outputs, these layers are called *Dense* layers]

.center[
.width-70[![](figures/lec1/multiOuptup.png)]
]

$$z\_j = \sum\_{n=1}^{m} w\_{j, n} x\_n  + b\_j$$

---

class: middle

## Example

.center[
.width-50[![](figures/lec1/multiOuptup.png)]
]
.smaller-xx[
$$\begin{aligned}
\mathbf{X}^{m \times 1} = \begin{bmatrix}
x\_1  \\\\
x\_2  \\\\
\vdots \\\\
x\_m
\end{bmatrix} 
&&
\mathbf{W}^{3 \times m} = \begin{bmatrix}
w\_{11} & w\_{12} &  \cdots & w\_{1m} \\\\
w\_{21} & w\_{22} & \cdots & w\_{2m} \\\\
w\_{31} & w\_{32} & \cdots & w\_{3m}
\end{bmatrix}
&& 
\mathbf{b}^{3 \times 1} = \begin{bmatrix}
b\_1 \\\\
b\_2 \\\\
b\_3
\end{bmatrix}
\end{aligned}$$

$$\boxed{\begin{aligned}
\mathbf{z} =  \mathbf{W} \cdot \mathbf{X} + \mathbf{b} 
&= \begin{bmatrix}
w\_{11} & w\_{12} &  \cdots & w\_{1m} \\\\
w\_{21} & w\_{22} & \cdots & w\_{2m} \\\\
w\_{31} & w\_{32} & \cdots & w\_{3m}
\end{bmatrix} \cdot
\begin{bmatrix}
x\_1  \\\\
x\_2  \\\\
\vdots \\\\
x\_m
\end{bmatrix} + 
\begin{bmatrix}
b\_1 \\\\
b\_2 \\\\
b\_3
\end{bmatrix} = \\\\
&= 
\begin{bmatrix}
w\_{11} x\_1 + w\_{12} x\_2 +  \cdots + w\_{1m} x\_m + b\_1 \\\\
w\_{21} x\_1 + w\_{22} x\_2 +  \cdots + w\_{2m} x\_m + b\_2 \\\\
w\_{31} x\_1 + w\_{32} x\_2 +  \cdots + w\_{3m} x\_m + b\_3 
\end{bmatrix} = \begin{bmatrix}
z\_1 \\\\
z\_2 \\\\
z\_3
\end{bmatrix}
\end{aligned}}$$

]

---




class: middle

.smaller-x[Because all inputs are densely connected to all outputs, these layers are called *Dense* layers]

.center[
.width-100[![](figures/lec1/multiOuptupTF.png)]
]

$$z\_j = \sum\_{n=1}^{m} w\_{j, n} x\_n  + b\_j$$

---

class: blue-slide, middle, center
count: false

.larger-xx[Multilayer Perceptron]

---

class: middle

# One hidden layer Neural Network

.center[
.width-100[![](figures/lec1/2layer.png)]
]

---

class: middle

# One hidden layer Neural Network

.center[
.width-100[![](figures/lec1/twoCode.png)]
]

---

class: middle

## One hidden layer Neural Network

.center[
.width-60[![](figures/lec1/2layer.png)]
]

.smaller-xx[
$$\begin{aligned}
\mathbf{X} = \begin{bmatrix}
x\_1  \\\\
x\_2  \\\\
x\_3
\end{bmatrix} 
&&
\mathbf{W}^{[1]} = \begin{bmatrix}
w\_{11} & w\_{12} &  w\_{13} \\\\
w\_{21} & w\_{22} &  w\_{23} \\\\
w\_{31} & w\_{32} &  w\_{33} \\\\
w\_{41} & w\_{42} &  w\_{43}
\end{bmatrix}
&& 
\mathbf{b}^{[1]} = \begin{bmatrix}
b\_1 \\\\
b\_2 \\\\
b\_3 \\\\
b\_4
\end{bmatrix}
&&
\mathbf{W}^{[2]} = \begin{bmatrix}
w\_{1} & w\_{2} &  w\_{3} & w\_{4} 
\end{bmatrix}
&& 
b^{[2]} = b
\end{aligned}$$


$$\boxed{\begin{aligned}
\mathbf{z}^{[1]} &= \mathbf{W}^{[1]} \cdot \mathbf{X} + \mathbf{b}^{[1]} \\\\
\mathbf{a}^{[1]} &= g^{[1]}(\mathbf{z}^{[1]}) \\\\
z^{[2]} &= \mathbf{W}^{[2]} \cdot \mathbf{a}^{[1]} + b^{[2]} \\\\
\hat y &= a^{[2]} = g^{[2]}(z^{[2]})
\end{aligned}}$$
]

---


class: middle

# Deep Neural Network

.center[
.width-100[![](figures/lec1/MLP2.png)]
]

---

class: blue-slide, middle, center
count: false

.larger-xx[Applying Neural Networks]

---

class: middle

.center[
.width-100[![](figures/lec1/e1.png)]
]

.footnote[Slide source: [MIT 6.S191](http://introtodeeplearning.com/)]

---

class: middle

.center[
.width-100[![](figures/lec1/e2.png)]
]

.footnote[Slide source: [MIT 6.S191](http://introtodeeplearning.com/)]

---

class: middle

.center[
.width-100[![](figures/lec1/e3.png)]
]

.footnote[Slide source: [MIT 6.S191](http://introtodeeplearning.com/)]

---

class: middle

.center[
.width-100[![](figures/lec1/e4.png)]
]

.footnote[Slide source: [MIT 6.S191](http://introtodeeplearning.com/)]

---

class: middle

.center[
.width-100[![](figures/lec1/e5.png)]
]

.footnote[Slide source: [MIT 6.S191](http://introtodeeplearning.com/)]

---

class: middle

.center[
.width-100[![](figures/lec1/e6.png)]
]

.footnote[Slide source: [MIT 6.S191](http://introtodeeplearning.com/)]

---

class: middle

.center[
.width-100[![](figures/lec1/e7.png)]
]

.footnote[Slide source: [MIT 6.S191](http://introtodeeplearning.com/)]

---

class: middle

.center[
.width-100[![](figures/lec1/e8.png)]
]

.footnote[Slide source: [MIT 6.S191](http://introtodeeplearning.com/)]

---

class: middle

.center[
.width-100[![](figures/lec1/e9.png)]
]

.footnote[Slide source: [MIT 6.S191](http://introtodeeplearning.com/)]

---


class: end-slide, center
count: false

.larger-xx[–ö—ñ–Ω–µ—Ü—å]


